![image-20200322193447008](https://tva1.sinaimg.cn/large/00831rSTly1gd2xq1h1irj311k0u0qca.jpg)

> DnCNN并非每隔两层就加一个shortcut connection，而是将网络的输出直接改成residual image（残差图片），设纯净图片为x，带噪音图片为y，假设y=x+v，则v是残差图片。即DnCNN的优化目标不是真实图片与网络输出之间的MSE(均方误差)，而是真实残差图片与网络输出之间的MSE。

之前的几个链接

**Image super-resolution reconstruction based on multi-scale feature loss function**

实验过程中缺乏改进的量化表现

https://zhuanlan.zhihu.com/p/38574562CNN图像处理常用损失函数对比评测

同样缺乏量化表现，主要解决失真问题

### 多尺度res2net

![image-20200323194818450](/Users/leyang/Library/Application Support/typora-user-images/image-20200323194818450.png)

传统的残差模块可以修改卷积为res2net的形式

![image-20200323200127896](/Users/leyang/Library/Application Support/typora-user-images/image-20200323200127896.png)

将其融合进senet

https://blog.csdn.net/ruoruojiaojiao/article/details/89074763、

# Multi-wavelet residual dense convolutional neural network for image denoising

![image-20200323203644242](/Users/leyang/Library/Application Support/typora-user-images/image-20200323203644242.png)

![image-20200323204424031](/Users/leyang/Library/Application Support/typora-user-images/image-20200323204424031.png)

> https://www.jianshu.com/p/3bf604f8b1cd
>
> 残差模块与dense模块相结合

本文提出的densenet就更霸道了，为了确保网络中最大的信息流通，让每层都与改层之前的所有层都相连，即每层的输入，是前面所有层的输出的concat.(resnet用的是sum).整体结构是酱紫的：





u-net：rednet

残差结构：DnCNN

可分离膨胀卷积：Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation

1. 在残差块中加入多尺度模块
2. TT-layer
3. deformable convolution



<hr/>

### 小波神经网络

以小波函数作为神经元的激活函数的前馈型神经网络

> 解决问题：BP算法的基本原理使得它不可避免地具有收敛速度慢、容易陷入局部极小值以及容易引起振荡等缺点

- Multi-level Wavelet Convolutional Neural Networks

![image-20200328201050887](/Users/leyang/Library/Application Support/typora-user-images/image-20200328201050887.png)

核心思想：小波变换嵌入CNN架构中，以降低特征图的分辨率，同时增加感受野

MWCNN与U-Net的区别：

对于上采样和下采样。U-Net中采用最大池化和上卷积（up-convolution）。而在MWCNN中采用DWT和IWT

解决问题：卷积网络（CNN）通常采用池化来扩大接收域，其优点是计算复杂度低。但是，合并可能导致信息丢失，因此不利于进一步的操作，例如特征提取和分析。

> DWT:小波变换
>
> IWT:逆小波变换

https://blog.csdn.net/gwplovekimi/article/details/84851871

### deformable convolution

CNNs对大型，未知形状变换的建模存在固有的缺陷

可变卷积主要应用于high level的任务中

应对形变对目标检测和语义分割等任务的影响

> 在输入的feature map中，原始的通过sliding window得到的是绿框，引入可变形卷积后，我们把原来的卷积网路分为两路，共享input feature map，其中上面的一路用一个额外的conv层来学习offset，得到H*W*2N的输出offset，其中，2N的意思是有x,y两个方向的偏移，得到这个之后，我们对原始卷积的每一个窗口，都不再是原来规整的sliding window（input feature map中的绿框）而是经过平移后的window(input feature map中的篮框)，取得数据后计算过程和常规卷积一样，即**input feature map和offset共同作为deformable conv层的输入。**

![image-20200328200330461](/Users/leyang/Library/Application Support/typora-user-images/image-20200328200330461.png)

对感受野增加便宜量

https://github.com/msracver/Deformable-ConvNets

https://www.zhihu.com/question/57493889/answer/184578752

### TT-layer

Tensorizing Neural Networks(神经网络张量化）

神经网络的参数之所以多主要是因为层与层间的权值。于是张量神经网络就是用**张量来存储权值**的神经网络。再进行张量分解就可以进行张量的压缩，进而节省存储空间，提高计算速度等

代码地址

https://github.com/timgaripov/TensorNet-TF

![image-20200328200519869](/Users/leyang/Library/Application Support/typora-user-images/image-20200328200519869.png)

https://github.com/z-bingo/Deformable-Kernels-For-Video-Denoising